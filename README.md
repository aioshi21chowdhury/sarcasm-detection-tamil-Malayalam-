# ğŸ§  Sarcasm Detection in Tamil & Malayalam â€“ Data Preprocessing  

### ğŸ” Project Overview  
This repository is part of a larger project aimed at **detecting sarcasm** in Tamil and Malayalam text using deep learning models. My contribution focuses on **data preprocessing**, which is crucial for improving model performance.  

### âœ¨ My Contribution  
I worked on cleaning and preprocessing the dataset to ensure high-quality input for the model. My tasks included:  
âœ” **Text Cleaning** â€“ Removing special characters, URLs, and unnecessary symbols  
âœ” **Tokenization** â€“ Splitting text into meaningful words  
âœ” **Stopword Removal** â€“ Eliminating common but unimportant words  
âœ” **Normalization** â€“ Handling variations in spelling and script differences  
âœ” **Word Embeddings Preparation** â€“ Converting text into numerical vectors for model input  

### ğŸ› ï¸ Preprocessing Steps  
```python
import re
import nltk
from nltk.tokenize import word_tokenize

# Load stopwords for Tamil & Malayalam
stopwords_ta = set(open("stopwords_tamil.txt").read().split())
stopwords_ml = set(open("stopwords_malayalam.txt").read().split())

def clean_text(text):
    text = text.lower()  
    text = re.sub(r"http\S+", "", text)  # Remove URLs
    text = re.sub(r"[^a-zA-Z\u0B80-\u0BFF\u0D00-\u0D7F\s]", "", text)  # Keep Tamil & Malayalam characters
    tokens = word_tokenize(text)  
    tokens = [word for word in tokens if word not in stopwords_ta and word not in stopwords_ml]  
    return " ".join(tokens)

# Example Usage
sample_text = "à®‡à®¨à¯à®¤ à®’à®°à¯ à®‰à®¤à®¾à®°à®£à®®à¯! ğŸ˜‚ http://example.com"
print(clean_text(sample_text))
```

### ğŸ“Š Impact of Preprocessing  
- **Reduced Noise**: Improved dataset quality by removing irrelevant content  
- **Better Model Training**: Enhanced word embeddings for sarcasm detection  
- **Efficient Tokenization**: Optimized text structure for deep learning models  

### ğŸ“Œ Acknowledgment  
This contribution is part of a larger **Sarcasm Detection in Tamil & Malayalam** project.  

ğŸ‘‰ *Excited to see how this dataset helps improve sarcasm detection!* ğŸš€
